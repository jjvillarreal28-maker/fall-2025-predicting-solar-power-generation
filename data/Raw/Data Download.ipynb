{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adjustable-calculation",
   "metadata": {},
   "source": [
    "## Data Download\n",
    "\n",
    "This notebook assists in downloading the relevant data.\n",
    "\n",
    "The PVDAQ Data Archives is in an Amazon S3 bucket, and has packages that assist in accessing the data sets. These sets are public, so require no API keys. The data can also be found through https://openei.org/wiki/PVDAQ/PVData_Map and downloaded manually. Due to the size of the environment data file and limitations in default Git storage, the raw file provided in this repository is a reduced version.\n",
    "\n",
    "The NSRDB database can be found at https://nsrdb.nrel.gov/data-viewer and requires an email to receive the chosen data. Also provided is a script that downloads the data; this requires additionally giving an API key.\n",
    "\n",
    "The .grib comes from the ERA5 dataset found at https://cds.climate.copernicus.eu/datasets/reanalysis-era5-single-levels?tab=download. There is an API that can assist in downloading, or a manual request can be made. Due to the size of possible data requests, cloud cover was considered a primary data point to collect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-paper",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import relevant packages and select site+download location\n",
    "import os\n",
    "import boto3\n",
    "import botocore\n",
    "from botocore.handlers import disable_signing\n",
    "\n",
    "site = \"9068\"\n",
    "path = './Data_9068'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-rough",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 9068_irradiance_data_20240101_20250430.csv\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.resource(\"s3\")\n",
    "s3.meta.client.meta.events.register(\"choose-signer.s3.*\", disable_signing)\n",
    "bucket = s3.Bucket(\"oedi-data-lake\")\n",
    "\n",
    "#Find each target file in buckets\n",
    "target_dir = site + '_OEDI'\n",
    "prefix =  \"pvdaq/2023-solar-data-prize/\" +  target_dir + \"/data/\"\n",
    "objects = bucket.objects.filter(Prefix=prefix)\n",
    "year = \"2024\"\n",
    "\n",
    "#Download chosen data files\n",
    "for obj in objects:\n",
    "    if year in obj.key[43:]:\n",
    "        bucket.download_file(obj.key, os.path.join(path, os.path.basename(obj.key)).replace(\"\\\\\", \"/\"))\n",
    "        print(\"Downloaded\", obj.key[43:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd8e7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python code provided by NSRDB\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import time\n",
    "\n",
    "API_KEY = \"{{YOUR_API_KEY}}\"\n",
    "EMAIL = \"insert.your.email@fake.com\"\n",
    "BASE_URL = \"https://developer.nrel.gov/api/nsrdb/v2/solar/nsrdb-GOES-conus-v4-0-0-download.json?\"\n",
    "POINTS = [\n",
    "'1770199'\n",
    "]\n",
    "\n",
    "def main():\n",
    "    input_data = {\n",
    "        'attributes': 'air_temperature,alpha,aod,asymmetry,clearsky_dhi,clearsky_dni,clearsky_ghi,cloud_fill_flag,cloud_type,dew_point,dhi,dni,fill_flag,ghi,ozone,relative_humidity,solar_zenith_angle,ssa,surface_albedo,surface_pressure,total_precipitable_water,wind_direction,wind_speed',\n",
    "        'interval': '5',\n",
    "        \n",
    "        'api_key': API_KEY,\n",
    "        'email': EMAIL,\n",
    "    }\n",
    "    for name in ['2024','2023']:\n",
    "        print(f\"Processing name: {name}\")\n",
    "        for id, location_ids in enumerate(POINTS):\n",
    "            input_data['names'] = [name]\n",
    "            input_data['location_ids'] = location_ids\n",
    "            print(f'Making request for point group {id + 1} of {len(POINTS)}...')\n",
    "\n",
    "            if '.csv' in BASE_URL:\n",
    "                url = BASE_URL + urllib.parse.urlencode(data, True)\n",
    "                # Note: CSV format is only supported for single point requests\n",
    "                # Suggest that you might append to a larger data frame\n",
    "                data = pd.read_csv(url)\n",
    "                print(f'Response data (you should replace this print statement with your processing): {data}')\n",
    "                # You can use the following code to write it to a file\n",
    "                # data.to_csv('SingleBigDataPoint.csv')\n",
    "            else:\n",
    "                headers = {\n",
    "                  'x-api-key': API_KEY\n",
    "                }\n",
    "                data = get_response_json_and_handle_errors(requests.post(BASE_URL, input_data, headers=headers))\n",
    "                download_url = data['outputs']['downloadUrl']\n",
    "                # You can do with what you will the download url\n",
    "                print(data['outputs']['message'])\n",
    "                print(f\"Data can be downloaded from this url when ready: {download_url}\")\n",
    "\n",
    "                # Delay for 1 second to prevent rate limiting\n",
    "                time.sleep(1)\n",
    "            print(f'Processed')\n",
    "\n",
    "\n",
    "def get_response_json_and_handle_errors(response: requests.Response) -> dict:\n",
    "    \"\"\"Takes the given response and handles any errors, along with providing\n",
    "    the resulting json\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    response : requests.Response\n",
    "        The response object\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        The resulting json\n",
    "    \"\"\"\n",
    "    if response.status_code != 200:\n",
    "        print(f\"An error has occurred with the server or the request. The request response code/status: {response.status_code} {response.reason}\")\n",
    "        print(f\"The response body: {response.text}\")\n",
    "        exit(1)\n",
    "\n",
    "    try:\n",
    "        response_json = response.json()\n",
    "    except:\n",
    "        print(f\"The response couldn't be parsed as JSON, likely an issue with the server, here is the text: {response.text}\")\n",
    "        exit(1)\n",
    "\n",
    "    if len(response_json['errors']) > 0:\n",
    "        errors = '\\n'.join(response_json['errors'])\n",
    "        print(f\"The request errored out, here are the errors: {errors}\")\n",
    "        exit(1)\n",
    "    return response_json\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee0f369",
   "metadata": {},
   "source": [
    "<a id='step3'></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_ds_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
